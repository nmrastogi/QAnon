{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f86a482-52e0-4278-a008-0882faa2d93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Rawnold_Reagan</td>\n",
       "      <td>The metric system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>GarudaDarkblack</td>\n",
       "      <td>First AR build. How'd I do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Sunsetmal</td>\n",
       "      <td>Gunsmiths were all booked. So I said fuck it, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>arma</td>\n",
       "      <td>TFW you need to visit the wife's son but he go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Gewehr</td>\n",
       "      <td>Peace was never an option</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226631</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>KuzoKevin</td>\n",
       "      <td>We, for supporting Trump, are extremists accor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226632</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>undine53</td>\n",
       "      <td>Biden Quietly Revokes Trump’s Ban on Chinese C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226633</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Danleif</td>\n",
       "      <td>Night is day. Time to wake up!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226634</th>\n",
       "      <td>thedonald</td>\n",
       "      <td>Awshit1</td>\n",
       "      <td>Big brother is always there to protek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226635</th>\n",
       "      <td>greatawakening</td>\n",
       "      <td>MikeWho</td>\n",
       "      <td>The people complaining about too many George p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226636 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  site           author  \\\n",
       "0        weekendgunnit   Rawnold_Reagan   \n",
       "1        weekendgunnit  GarudaDarkblack   \n",
       "2        weekendgunnit        Sunsetmal   \n",
       "3        weekendgunnit             arma   \n",
       "4        weekendgunnit           Gewehr   \n",
       "...                ...              ...   \n",
       "226631       thedonald        KuzoKevin   \n",
       "226632  greatawakening         undine53   \n",
       "226633       thedonald          Danleif   \n",
       "226634       thedonald          Awshit1   \n",
       "226635  greatawakening          MikeWho   \n",
       "\n",
       "                                                    title  \n",
       "0                                       The metric system  \n",
       "1                             First AR build. How'd I do?  \n",
       "2       Gunsmiths were all booked. So I said fuck it, ...  \n",
       "3       TFW you need to visit the wife's son but he go...  \n",
       "4                               Peace was never an option  \n",
       "...                                                   ...  \n",
       "226631  We, for supporting Trump, are extremists accor...  \n",
       "226632  Biden Quietly Revokes Trump’s Ban on Chinese C...  \n",
       "226633                     Night is day. Time to wake up!  \n",
       "226634              Big brother is always there to protek  \n",
       "226635  The people complaining about too many George p...  \n",
       "\n",
       "[226636 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"../largedata/new_range/dotwin_posts.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a622bbc-4ace-49f5-a3aa-6cf0672af64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        {'fusion': 360, 'violence': 546, 'identificati...\n",
      "1        {'fusion': 360, 'violence': 546, 'identificati...\n",
      "2        {'fusion': 360, 'violence': 546, 'identificati...\n",
      "3        {'fusion': 360, 'violence': 546, 'identificati...\n",
      "4        {'fusion': 360, 'violence': 546, 'identificati...\n",
      "                               ...                        \n",
      "29995    {'fusion': 360, 'violence': 546, 'identificati...\n",
      "29996    {'fusion': 360, 'violence': 546, 'identificati...\n",
      "29997    {'fusion': 360, 'violence': 546, 'identificati...\n",
      "29998    {'fusion': 360, 'violence': 546, 'identificati...\n",
      "29999    {'fusion': 360, 'violence': 546, 'identificati...\n",
      "Name: tokens, Length: 30000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the CSV file\n",
    "file_path_posts = \"../largedata/new_range/dotwin_posts.csv\"\n",
    "posts_data = pd.read_csv(file_path_posts).head(30000)\n",
    "\n",
    "# NLTK English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Full dictionary of terms (as provided by you)\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill us\", \"kill our\", \"kill my\", \"running out of time\", \"run out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "\n",
    "# Dictionary of categories\n",
    "extremism_dictionary = {\n",
    "    \"fusion\": fusion,\n",
    "    \"violence\": violence,\n",
    "    \"identification1\": identification1,\n",
    "    \"identification2\": identification2,\n",
    "    \"slurs\": slurs,\n",
    "    \"demonisation\": demonisation,\n",
    "    \"dehumanisation\": dehumanisation,\n",
    "    \"existential_threat\": existential_threat,\n",
    "    \"conspiracy\": conspiracy,\n",
    "    \"inevitable_war1\": inevitable_war1,\n",
    "    \"inevitable_war2\": inevitable_war2,\n",
    "    \"violence_justification\": violence_justification,\n",
    "    \"martyr\": martyr,\n",
    "    \"violent_role_model1\": violent_role_model1,\n",
    "    \"violent_role_model2\": violent_role_model2,\n",
    "    \"hopelessness1\": hopelessness1,\n",
    "    \"hopelessness2\": hopelessness2\n",
    "}\n",
    "\n",
    "# Function to clean and tokenize text using regex instead of nltk tokenizers\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):  # Handle non-string data\n",
    "        return []\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text).lower()\n",
    "    # Tokenize the text using regex (split by whitespace)\n",
    "    tokens = re.split(r'\\s+', text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess and tokenize the 'title' column\n",
    "posts_data['tokens'] = posts_data['title'].apply(preprocess_text)\n",
    "\n",
    "# Create a CountVectorizer instance with the preprocessed tokens\n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "dfm = vectorizer.fit_transform(posts_data['tokens'])\n",
    "\n",
    "# Convert the DFM into a DataFrame for easier processing\n",
    "dfm_df = pd.DataFrame(dfm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Function to match tokens to dictionary terms and count frequencies\n",
    "def match_dictionary_terms(dfm, dictionary):\n",
    "    matched_terms = {}\n",
    "    for category, terms in dictionary.items():\n",
    "        category_count = 0\n",
    "        for term in terms:\n",
    "            term = term.replace(\"\\\\b\", \"\")  # Remove boundary markers for matching\n",
    "            if term in dfm.columns:\n",
    "                category_count += dfm[term].sum()\n",
    "        matched_terms[category] = category_count\n",
    "    return matched_terms\n",
    "\n",
    "# Apply the matching function to get term frequencies for each category\n",
    "matched_results = posts_data['tokens'].apply(lambda tokens: match_dictionary_terms(dfm_df, extremism_dictionary))\n",
    "\n",
    "# Display the results\n",
    "print(matched_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15695df3-3f05-4e9c-b845-d3a8199946b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'fusion': 360, 'violence': 546, 'identificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'fusion': 360, 'violence': 546, 'identificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'fusion': 360, 'violence': 546, 'identificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'fusion': 360, 'violence': 546, 'identificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'fusion': 360, 'violence': 546, 'identificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>{'fusion': 360, 'violence': 546, 'identificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>{'fusion': 360, 'violence': 546, 'identificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>{'fusion': 360, 'violence': 546, 'identificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>{'fusion': 360, 'violence': 546, 'identificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>{'fusion': 360, 'violence': 546, 'identificati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tokens\n",
       "0      {'fusion': 360, 'violence': 546, 'identificati...\n",
       "1      {'fusion': 360, 'violence': 546, 'identificati...\n",
       "2      {'fusion': 360, 'violence': 546, 'identificati...\n",
       "3      {'fusion': 360, 'violence': 546, 'identificati...\n",
       "4      {'fusion': 360, 'violence': 546, 'identificati...\n",
       "...                                                  ...\n",
       "29995  {'fusion': 360, 'violence': 546, 'identificati...\n",
       "29996  {'fusion': 360, 'violence': 546, 'identificati...\n",
       "29997  {'fusion': 360, 'violence': 546, 'identificati...\n",
       "29998  {'fusion': 360, 'violence': 546, 'identificati...\n",
       "29999  {'fusion': 360, 'violence': 546, 'identificati...\n",
       "\n",
       "[30000 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=pd.DataFrame(matched_results)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a941ca08-4692-49a4-be7b-57f81698b78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fusion': 360,\n",
       " 'violence': 546,\n",
       " 'identification1': 1302,\n",
       " 'identification2': 0,\n",
       " 'slurs': 189,\n",
       " 'demonisation': 518,\n",
       " 'dehumanisation': 219,\n",
       " 'existential_threat': 168,\n",
       " 'conspiracy': 700,\n",
       " 'inevitable_war1': 506,\n",
       " 'inevitable_war2': 730,\n",
       " 'violence_justification': 129,\n",
       " 'martyr': 31,\n",
       " 'violent_role_model1': 39,\n",
       " 'violent_role_model2': 231,\n",
       " 'hopelessness1': 484,\n",
       " 'hopelessness2': 190}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"tokens\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bed7186-f22e-4697-9b3c-0b45b2bda3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matched_terms\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Apply the matching function to get term frequencies for each category\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m matched_results \u001b[38;5;241m=\u001b[39m \u001b[43mposts_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_dictionary_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfm_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextremism_dictionary\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4774\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4665\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4666\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4669\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4670\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4671\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4672\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4673\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4772\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4773\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1100\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1151\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1151\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2919\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn [26], line 95\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matched_terms\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Apply the matching function to get term frequencies for each category\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m matched_results \u001b[38;5;241m=\u001b[39m posts_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m tokens: \u001b[43mmatch_dictionary_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfm_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextremism_dictionary\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn [26], line 90\u001b[0m, in \u001b[0;36mmatch_dictionary_terms\u001b[0;34m(dfm, dictionary)\u001b[0m\n\u001b[1;32m     88\u001b[0m         term \u001b[38;5;241m=\u001b[39m term\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Remove boundary markers for matching\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m dfm\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m---> 90\u001b[0m             category_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mdfm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     matched_terms[category] \u001b[38;5;241m=\u001b[39m category_count\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m matched_terms\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:11806\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.sum\u001b[0;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  11786\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m  11787\u001b[0m     _num_doc,\n\u001b[1;32m  11788\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the sum of the values over the requested axis.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11804\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11805\u001b[0m ):\n\u001b[0;32m> 11806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11807\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  11808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:11508\u001b[0m, in \u001b[0;36mNDFrame.sum\u001b[0;34m(self, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  11499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\n\u001b[1;32m  11500\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  11501\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11506\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11507\u001b[0m ):\n\u001b[0;32m> 11508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_min_count_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11509\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnansum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  11510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:11490\u001b[0m, in \u001b[0;36mNDFrame._min_count_stat_function\u001b[0;34m(self, name, func, axis, skipna, level, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m  11474\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m  11475\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the level keyword in DataFrame and Series aggregations is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m  11476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in a future version. Use groupby \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11479\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()),\n\u001b[1;32m  11480\u001b[0m     )\n\u001b[1;32m  11481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_by_level(\n\u001b[1;32m  11482\u001b[0m         name,\n\u001b[1;32m  11483\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11487\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m  11488\u001b[0m     )\n\u001b[0;32m> 11490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  11491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11493\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  11497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4819\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4815\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   4816\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not implement \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4817\u001b[0m     )\n\u001b[1;32m   4818\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/nanops.py:93\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/nanops.py:418\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 418\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[1;32m    421\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/nanops.py:491\u001b[0m, in \u001b[0;36mmaybe_operate_rowwise.<locals>.newfunc\u001b[0;34m(values, axis, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         results \u001b[38;5;241m=\u001b[39m [func(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrs]\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(results)\n\u001b[0;32m--> 491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/nanops.py:631\u001b[0m, in \u001b[0;36mnansum\u001b[0;34m(values, axis, skipna, min_count, mask)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_timedelta64_dtype(dtype):\n\u001b[1;32m    629\u001b[0m     dtype_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m--> 631\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _maybe_null_out(the_sum, axis, mask, values\u001b[38;5;241m.\u001b[39mshape, min_count\u001b[38;5;241m=\u001b[39mmin_count)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m the_sum\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:48\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the CSV file\n",
    "file_path_posts = \"../largedata/new_range/dotwin_posts.csv\"\n",
    "posts_data = pd.read_csv(file_path_posts)\n",
    "# NLTK English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Full dictionary of terms (as provided by you)\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill us\", \"kill our\", \"kill my\", \"running out of time\", \"run out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "\n",
    "# Dictionary of categories\n",
    "extremism_dictionary = {\n",
    "    \"fusion\": fusion,\n",
    "    \"violence\": violence,\n",
    "    \"identification1\": identification1,\n",
    "    \"identification2\": identification2,\n",
    "    \"slurs\": slurs,\n",
    "    \"demonisation\": demonisation,\n",
    "    \"dehumanisation\": dehumanisation,\n",
    "    \"existential_threat\": existential_threat,\n",
    "    \"conspiracy\": conspiracy,\n",
    "    \"inevitable_war1\": inevitable_war1,\n",
    "    \"inevitable_war2\": inevitable_war2,\n",
    "    \"violence_justification\": violence_justification,\n",
    "    \"martyr\": martyr,\n",
    "    \"violent_role_model1\": violent_role_model1,\n",
    "    \"violent_role_model2\": violent_role_model2,\n",
    "    \"hopelessness1\": hopelessness1,\n",
    "    \"hopelessness2\": hopelessness2\n",
    "}\n",
    "\n",
    "# Function to clean and tokenize text using regex instead of nltk tokenizers\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):  # Handle non-string data\n",
    "        return []\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text).lower()\n",
    "    # Tokenize the text using regex (split by whitespace)\n",
    "    tokens = re.split(r'\\s+', text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess and tokenize the 'title' column\n",
    "posts_data['tokens'] = posts_data['title'].apply(preprocess_text)\n",
    "\n",
    "# Create a CountVectorizer instance with the preprocessed tokens\n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "dfm = vectorizer.fit_transform(posts_data['tokens'])\n",
    "\n",
    "# Convert the DFM into a DataFrame for easier processing\n",
    "dfm_df = pd.DataFrame(dfm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Function to match tokens to dictionary terms and count frequencies\n",
    "def match_dictionary_terms(dfm, dictionary):\n",
    "    matched_terms = {}\n",
    "    for category, terms in dictionary.items():\n",
    "        category_count = 0\n",
    "        for term in terms:\n",
    "            term = term.replace(\"\\\\b\", \"\")  # Remove boundary markers for matching\n",
    "            if term in dfm.columns:\n",
    "                category_count += dfm[term].sum()\n",
    "        matched_terms[category] = category_count\n",
    "    return matched_terms\n",
    "\n",
    "# Apply the matching function to get term frequencies for each category\n",
    "matched_results = posts_data['tokens'].apply(lambda tokens: match_dictionary_terms(dfm_df, extremism_dictionary))\n",
    "\n",
    "# Display the results\n",
    "# print(matched_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dcdeefa-8efd-40ac-babd-c0f9f0ac3028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Rawnold_Reagan</td>\n",
       "      <td>The metric system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>GarudaDarkblack</td>\n",
       "      <td>First AR build. How'd I do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Sunsetmal</td>\n",
       "      <td>Gunsmiths were all booked. So I said fuck it, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>arma</td>\n",
       "      <td>TFW you need to visit the wife's son but he go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>Gewehr</td>\n",
       "      <td>Peace was never an option</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>xxxMAGA420xxx</td>\n",
       "      <td>If you shake it more than twice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>mojavegreen</td>\n",
       "      <td>Life Goals, no Toes. Doggo, rifle, cigar, only...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>memegunnie</td>\n",
       "      <td>my masterpiece a chicken-gun!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>JoshHawley2024</td>\n",
       "      <td>Sunday Gunday: Squirrel riffle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>weekendgunnit</td>\n",
       "      <td>RhodesianCoonBasher</td>\n",
       "      <td>He's a Spy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site               author  \\\n",
       "0  weekendgunnit       Rawnold_Reagan   \n",
       "1  weekendgunnit      GarudaDarkblack   \n",
       "2  weekendgunnit            Sunsetmal   \n",
       "3  weekendgunnit                 arma   \n",
       "4  weekendgunnit               Gewehr   \n",
       "5  weekendgunnit        xxxMAGA420xxx   \n",
       "6  weekendgunnit          mojavegreen   \n",
       "7  weekendgunnit           memegunnie   \n",
       "8  weekendgunnit       JoshHawley2024   \n",
       "9  weekendgunnit  RhodesianCoonBasher   \n",
       "\n",
       "                                               title  \n",
       "0                                  The metric system  \n",
       "1                        First AR build. How'd I do?  \n",
       "2  Gunsmiths were all booked. So I said fuck it, ...  \n",
       "3  TFW you need to visit the wife's son but he go...  \n",
       "4                          Peace was never an option  \n",
       "5                 If you shake it more than twice...  \n",
       "6  Life Goals, no Toes. Doggo, rifle, cigar, only...  \n",
       "7                      my masterpiece a chicken-gun!  \n",
       "8                     Sunday Gunday: Squirrel riffle  \n",
       "9                                         He's a Spy  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"../largedata/new_range/dotwin_posts.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f90bc3a3-46c8-45fb-95af-b9ab7fa6fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the CSV file\n",
    "file_path_posts = \"../largedata/new_range/dotwin_posts.csv\"\n",
    "posts_data = pd.read_csv(file_path_posts).head(10)\n",
    "# NLTK English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Full dictionary of terms (as provided by you)\n",
    "fusion = [\"brother\", \"sister\", \"family\", \"motherland\", \"our blood\", \"fatherland\", \"sons\", \"daughters\", \"kin\", \"my people\", \"my race\", \"our people\", \"European race\", \"ancestry\", \"ancestor\", \"descendant\", \"fellow\", \"brethren\", \"comrades\"]\n",
    "violence = [\"kill\", \"hang\", \"bomb\", \"shoot\", \"slaughter\", \"execute\", \"execution\", \"punish\", \"death penalty\", \"massacre\", \"destroy\", \"must attack\", \"must fight\", \"revenge\", \"retribution\", \"eradicate\", \"starve\", \"die\", \"torture\", \"behead\", \"burn\", \"bring death to\", \"give them hell\", \"weapon\", \"firearm\", \"assassinate\", \"gun\", \"rifle\", \"knife\", \"grenade\", \"brutal steps\", \"molotov\", \"jihaad\", \"jihad\", \"set fire\", \"revolution\", \"forcible overthrow\", \"flamethrowers\", \"M1-16\", \"ammonium nitrate\"]\n",
    "identification1 = [\"\\\\bwe\\\\b\", \"\\\\bus\\\\b\", \"\\\\bour\\\\b\", \"\\\\bthey\\\\b\", \"\\\\bthem\\\\b\", \"\\\\btheir\\\\b\"]\n",
    "identification2 = [\"\\\\bI\\\\b\", \"\\\\bme\\\\b\", \"\\\\bmy\\\\b\", \"\\\\byou\\\\b\", \"\\\\byour\\\\b\"]\n",
    "slurs = [\"kike\", \"nigger\", \"negro\", \"dirty jew\", \"spic\", \"fag\", \"goyim\", \"golem\", \"the jew\", \"global jewry\", \"pajeet\", \"bitch\", \"whore\"]\n",
    "demonisation = [\"traitor\", \"evil\", \"enemy\", \"corrupt\", \"vicious\", \"barbaric\", \"depraved\", \"vile\", \"puppets\", \"perversion\", \"blood libel\", \"pervert\", \"pedo\", \"crime\", \"cruel\", \"bloody\", \"genocidal\", \"sinful\", \"deceitful\", \"invader\", \"poison\", \"parasite\", \"menace\", \"brutal\", \"ruthless\", \"bloodsucking\", \"dirty\", \"deceptive\", \"treacherous\", \"poisonous\", \"oppressive\", \"oppressor\", \"shird\", \"unbeliever\", \"immoral\", \"jahili\", \"pollute\", \"demolish\", \"shake the foundations\", \"dar ul-harb\", \"arrogant\", \"mischievous\", \"criminal\", \"deceivers\", \"liars\"]\n",
    "dehumanisation = [\"animal\", \"plague\", \"impure\", \"brute\", \"dog\", \"lower iq\", \"lower being\", \"inferior\", \"squalid\", \"parasitic\", \"parasite\", \"creature\", \"trash\", \"filth\", \"vermin\", \"spider\", \"devil\", \"monster\", \"beast\", \"reptile\", \"reptiloid\", \"femoid\", \"reptilian\", \"snake\", \"cockroach\", \"beneath human skin\", \"sub human\", \"anti-human\", \"disease\", \"savage\", \"infest\", \"breed\", \"locust\", \"monkey\", \"gorilla\", \"rat\", \"microbe\", \"satan\", \"cancer\", \"scum\"]\n",
    "existential_threat = [\"subjected to\", \"coerced\", \"brainwashed\", \"exterminated\", \"brutalised\", \"raped\", \"terrorised\", \"ravaged\", \"extinction\", \"replacement\", \"genocide\", \"robbed\", \"subjugate\", \"make war upon my people\", \"destroy\", \"subvert\", \"overwhelmed\", \"under siege\", \"demographic siege\", \"disenfranchise\", \"assault\", \"kill us\", \"kill our\", \"kill my\", \"running out of time\", \"run out of time\", \"last chance\", \"enslavement\", \"enslaved\", \"suffer\", \"plunder\", \"condemned to death\", \"destruction of all mankind\", \"at the brink of\", \"endanger\", \"annihilation\", \"decay\"]\n",
    "conspiracy = [\"betray\", \"betrayal\", \"sell\", \"sold\", \"collude\", \"conspire\", \"fake\", \"fraud\", \"corruption\", \"corrupt\", \"zog\", \"great replacement\", \"white genocide\", \"kalergi\", \"pedo elites\", \"NWO\", \"illuminati\", \"inside job\", \"Eurabia\"]\n",
    "inevitable_war1 = [\"war\", \"battle\", \"fight\", \"jihad\", \"jihaad\", \"collapse\", \"conflict\"]\n",
    "inevitable_war2 = [\"imminent\", \"inevitable\", \"looming\", \"start\", \"begin\", \"already\", \"heading for\", \"ongoing\", \"stage\", \"phase\", \"when\", \"has been\", \"likely\", \"predict\", \"expect\", \"will happen\", \"has begun\", \"current\", \"impending\"]\n",
    "violence_justification = [\"pre-emptive\", \"defend\", \"protect\", \"self-defense\", \"self-defence\", \"forced to fight\", \"no longer ignore\", \"act of defense\", \"purified\", \"purify\", \"need for war\", \"need for violence\", \"need for jihad\", \"struggle is imposed\", \"natural struggle\", \"cannot co-exist\"]\n",
    "martyr = [\"die in glory\", \"sacrifice\", \"knight\", \"martyr\", \"die selflessly\", \"protecting our people\", \"immortal\", \"preserve\", \"act of preservation\", \"defend the world of the Lord\", \"defending the work of the Lord\", \"stand guard\", \"standing guard\", \"the herald\", \"release mankind from\", \"free from\", \"freed from\"]\n",
    "violent_role_model1 = [\"breivik\", \"tarrant\", \"hitler\", \"crusius\", \"rodger\", \"baillet\", \"earnest\", \"minassian\", \"mcveigh\", \"christchurch\", \"poway\", \"el paso\"]\n",
    "violent_role_model2 = [\"hero\", \"role model\", \"saint\", \"inspire\", \"inspiration\", \"inspiring\", \"support\", \"influenced\"]\n",
    "hopelessness1 = [\"democracy\", \"democratic\", \"peaceful\", \"political\", \"system\", \"politics\", \"dialogue\", \"passivity\"]\n",
    "hopelessness2 = [\"meaningless\", \"weak\", \"fail\", \"end\", \"vanish\", \"man-made\", \"flawed\", \"jahili\", \"given up\"]\n",
    "\n",
    "# Dictionary of categories\n",
    "extremism_dictionary = {\n",
    "    \"fusion\": fusion,\n",
    "    \"violence\": violence,\n",
    "    \"identification1\": identification1,\n",
    "    \"identification2\": identification2,\n",
    "    \"slurs\": slurs,\n",
    "    \"demonisation\": demonisation,\n",
    "    \"dehumanisation\": dehumanisation,\n",
    "    \"existential_threat\": existential_threat,\n",
    "    \"conspiracy\": conspiracy,\n",
    "    \"inevitable_war1\": inevitable_war1,\n",
    "    \"inevitable_war2\": inevitable_war2,\n",
    "    \"violence_justification\": violence_justification,\n",
    "    \"martyr\": martyr,\n",
    "    \"violent_role_model1\": violent_role_model1,\n",
    "    \"violent_role_model2\": violent_role_model2,\n",
    "    \"hopelessness1\": hopelessness1,\n",
    "    \"hopelessness2\": hopelessness2\n",
    "}\n",
    "\n",
    "# Function to clean and tokenize text using regex instead of nltk tokenizers\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):  # Handle non-string data\n",
    "        return []\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text).lower()\n",
    "    # Tokenize the text using regex (split by whitespace)\n",
    "    tokens = re.split(r'\\s+', text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Preprocess and tokenize the 'title' column\n",
    "posts_data['tokens'] = posts_data['title'].apply(preprocess_text)\n",
    "\n",
    "# Create a CountVectorizer instance with the preprocessed tokens\n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "dfm = vectorizer.fit_transform(posts_data['tokens'])\n",
    "\n",
    "# Convert the DFM into a DataFrame for easier processing\n",
    "dfm_df = pd.DataFrame(dfm.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Function to match tokens to dictionary terms and count frequencies\n",
    "def match_dictionary_terms(dfm, dictionary):\n",
    "    matched_terms = {}\n",
    "    for category, terms in dictionary.items():\n",
    "        category_count = 0\n",
    "        for term in terms:\n",
    "            term = term.replace(\"\\\\b\", \"\")  # Remove boundary markers for matching\n",
    "            if term in dfm.columns:\n",
    "                category_count += dfm[term].sum()\n",
    "        matched_terms[category] = category_count\n",
    "    return matched_terms\n",
    "\n",
    "# Apply the matching function to get term frequencies for each category\n",
    "matched_results = posts_data['tokens'].apply(lambda tokens: match_dictionary_terms(dfm_df, extremism_dictionary))\n",
    "\n",
    "# Display the results\n",
    "# print(matched_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e39708c2-4760-4638-8b56-64bddc3cd7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fusion': 0,\n",
       " 'violence': 1,\n",
       " 'identification1': 0,\n",
       " 'identification2': 0,\n",
       " 'slurs': 0,\n",
       " 'demonisation': 0,\n",
       " 'dehumanisation': 0,\n",
       " 'existential_threat': 0,\n",
       " 'conspiracy': 0,\n",
       " 'inevitable_war1': 0,\n",
       " 'inevitable_war2': 0,\n",
       " 'violence_justification': 0,\n",
       " 'martyr': 0,\n",
       " 'violent_role_model1': 0,\n",
       " 'violent_role_model2': 0,\n",
       " 'hopelessness1': 1,\n",
       " 'hopelessness2': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12db0936-115b-405a-bac9-1e7590c758bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    The metric system\n",
       "1                          First AR build. How'd I do?\n",
       "2    Gunsmiths were all booked. So I said fuck it, ...\n",
       "3    TFW you need to visit the wife's son but he go...\n",
       "4                            Peace was never an option\n",
       "5                   If you shake it more than twice...\n",
       "6    Life Goals, no Toes. Doggo, rifle, cigar, only...\n",
       "7                        my masterpiece a chicken-gun!\n",
       "8                       Sunday Gunday: Squirrel riffle\n",
       "9                                           He's a Spy\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=df[\"title\"].head(10)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b77a7e7-cadf-4eb4-9589-2077d1d8206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_csv_rows(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        row_count = sum(1 for row in file) - 1  # Subtract 1 for the header row\n",
    "    return row_count\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"../largedata/new_range/parler_huge.csv\"\n",
    "total_rows = count_csv_rows(file_path)\n",
    "print(f\"Total rows in CSV (excluding header): {total_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76962f5b-e45a-4790-8f91-5ba323d77b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
